{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5a5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 15:19:10.719765: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#There code that I used from my references. Libraries used came from sources mainly sources 1,10,17,18\n",
    "#Most other coding sources listed in reference I also used. Note that there may be miscitations so I listed a reference at the bottom of the code\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9382024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code in this box came from sources 3, 5, 7. I have used these tutorials mainly\n",
    "device = torch.device('cuda' if torch.cuda.is_available()else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54258bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code in this box came from source 3, 5, 7. I have used these tutorials mainly\n",
    "data_transformer = {\n",
    "    'train': transforms.Compose([#training\n",
    "        transforms.Resize((150,150)),#pixel 224\n",
    "        transforms.RandomHorizontalFlip(),#flip\n",
    "        transforms.ToTensor(),#tensor accept tensor - convert tensor\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])#normalize data\n",
    "    ]),\n",
    "    'test':transforms.Compose([#testing\n",
    "        transforms.Resize((150,150)),\n",
    "        transforms.CenterCrop(150),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ]),\n",
    "    \n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d630b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code came from sources 3,5,7,19. Dataset came from source 6\n",
    "data_dir = '/Users/andyxiao/COEN140/LuggageDetectionProject'\n",
    "#data loaders\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x),data_transformer[x])for x in['train','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db718773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 88, 'test': 22}\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "#sources/code used 3,5,7,19. Dataset came from source 6\n",
    "dataloader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)for x in ['train','test']}\n",
    "dataset_sizes = {x: len(image_datasets[x])for x in['train', 'test']}\n",
    "print(dataset_sizes)\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79203358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources/code used 3,5,7. Dataset came from source 6\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder('/Users/andyxiao/COEN140/LuggageDetectionProject/test',transform=data_transformer['test']),\n",
    "    batch_size=9,shuffle=True)\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder('/Users/andyxiao/COEN140/LuggageDetectionProject/train',transform=data_transformer['train']),\n",
    "    batch_size=3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395462b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'luggage']\n"
     ]
    }
   ],
   "source": [
    "#sources/code used 3,5,7,19. Dataset came from source 6\n",
    "#categorites\n",
    "root = pathlib.Path('/Users/andyxiao/COEN140/LuggageDetectionProject/train')\n",
    "classes =sorted([j.name.split('/')[-1]for j in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1168ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources/code used from 3,4, 7. I modified some of the layers, kernel size, and number of layers etc... for both functions\n",
    "#CNN model with 4 layers and 30 output channels with a kernel size of 3\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super(CNN,self).__init__()\n",
    "        #output = w-f+2p/s +1 w= 150, f = 3, P = 1, s =1\n",
    "        #Random CNN\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3,stride=1,padding=1)\n",
    "        #shape = 256,12,150,150\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 6)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 3)\n",
    "        #shape = 256,12,50,50\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3,stride=1,padding=1)\n",
    "        #shape = 256,24,50,50\n",
    "        self.bn2 = nn.BatchNorm2d(num_features = 12)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3,stride=1,padding=1)\n",
    "        #shape = 256,24,50,50\n",
    "        self.bn3 = nn.BatchNorm2d(num_features = 24)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=30, kernel_size=3,stride=1,padding=1)\n",
    "        #shape = 256,24,50,50\n",
    "        self.bn4 = nn.BatchNorm2d(num_features = 30)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        #self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3,stride=1,padding=1)\n",
    "        #self.bn3 = nn.BatchNorm2d(num_features = 20)\n",
    "        #self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=30*50*50,out_features = num_classes)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "        \n",
    "        output = self.pool(output)\n",
    "        \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "        \n",
    "        output=self.conv3(output)\n",
    "        output=self.relu3(output)\n",
    "        \n",
    "        output=self.conv4(output)\n",
    "        output=self.bn4(output)\n",
    "        output=self.relu4(output)\n",
    "        \n",
    "        #output=self.conv3(input)\n",
    "        #output=self.bn3(output)\n",
    "        #output=self.relu3(output)\n",
    "        \n",
    "        \n",
    "        output=output.view(-1,30*50*50)\n",
    "        \n",
    "        output=self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6207e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (conv4): Conv2d(24, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu4): ReLU()\n",
      "  (fc): Linear(in_features=75000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#sources/code used from sources 3,5,7.\n",
    "model = CNN(num_classes=2).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1ed6bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#sources/code used 3,5,7.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#sources/code used 3,5,7.\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598962fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources 3,5,7,9\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code came from sources 3,5,7. dataset comes from source 6\n",
    "train_count = len(glob.glob('/Users/andyxiao/COEN140/LuggageDetectionProject/train'+'/*.jpg'))\n",
    "test_count = len(glob.glob('/Users/andyxiao/COEN140/LuggageDetectionProject/test'+'/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6513c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code came from sources 3,5,7\n",
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0329bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0335 Acc: 0.9773\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n",
      "train Loss: 0.0000 Acc: 1.0000\n",
      "test Loss: 0.0000 Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#code/sources came from sources 3,4,5,7,9.\n",
    "#train model with 10 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        running_loss = 0.0 #store loss\n",
    "        running_correct = 0.0 #store correct\n",
    "        \n",
    "        for inputs, labels in dataloader[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)#model on device\n",
    "            \n",
    "            optimizer.zero_grad()#clear gradients from previous iteration\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward() #backward pass update the weights after gradient calculation\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_correct += torch.sum(preds==labels.data)\n",
    "        \n",
    "        epochs_loss = running_loss/dataset_sizes[phase]\n",
    "        epochs_acc = running_correct.double()/dataset_sizes[phase]\n",
    "        \n",
    "        print(f'{phase} Loss: {epochs_loss:.4f} Acc: {epochs_acc:.4f}')\n",
    "        \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65d3a21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c9230e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/Users/andyxiao/COEN140/LuggageDetectionProject.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources/code used 8,12,14,15,16\n",
    "from PIL import Image\n",
    "\n",
    "image_path = ''\n",
    "image = Image.open(image_path)\n",
    "input_tensor = data_transformer['test'](image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "_, predicted_class = output.max(1)\n",
    "class_names = ['luggage']\n",
    "predicted_class = class_names[predicted_class.item()]\n",
    "print(f'the predicted class is: {predicted_class}')\n",
    "\n",
    "if(predicted_class=='luggage'):\n",
    "    AirportVerify(image_path)\n",
    "    print(\"Baggage is checked successfully and will be sent for flight verification\")\n",
    "else:\n",
    "    print(\"This is not an acceptable baggage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "386a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources/code used 8,12,14,15,16,20\n",
    "def AirportVerify(img):\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    if(text == 'SEA'):\n",
    "        print(\"Go head\")\n",
    "    else:\n",
    "        print(\"this goes to flight \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b24708f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readcsv():\n",
    "    import pandas as pd\n",
    "    file = pd.read_csv('/Users/andyxiao/Downloads/Book 7.txt')\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3e4719",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readcsv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rd \u001b[38;5;241m=\u001b[39mreadcsv()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#rd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(rd)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m Y \u001b[38;5;241m=\u001b[39m rd\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readcsv' is not defined"
     ]
    }
   ],
   "source": [
    "rd =readcsv()\n",
    "#rd\n",
    "#print(rd)\n",
    "Y = rd.iloc[:,-1]\n",
    "print(\"y \", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aff27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "References:\n",
    "\n",
    "1.   MISHRA, P. (2023). Chapters 3 and 9. In Pytorch recipes: A problem-solution approach to build, train and deploy neural network models (2nd ed.). essay, APRESS.\n",
    "2    AarohiSingla, A. (2023, October 13). Image-classification-using-pytorch: Image classification on custom dataset. GitHub. https://github.com/AarohiSingla/Image-Classification-Using-Pytorch\n",
    "3.   AI-SPECIALS, A.-S., & gaurav67890, gaurav67890. (2020, June 30). Image classification using CNN from scratch in pytorch- part 1 training. YouTube. https://www.youtube.com/watch?v=9OHlgDjaE2I\n",
    "4.   Androbomb, A. (2019, December 11). Using CNN to classify images w/pytorch. Kaggle. https://www.kaggle.com/code/androbomb/using-cnn-to-classify-images-w-pytorch/notebook\n",
    "5.   Code With Aarohii, C. W. A. (2023, October 13). Image classification using Pytorch and Convolutional Neural Network. YouTube. https://www.youtube.com/watch?v=cJpwQJp9flU\n",
    "6.   DataCluster Labs, D. L. (2023, January 19). Suitcase/Luggage Dataset Indoor Object Image. Kaggle. https://www.kaggle.com/datasets/dataclusterlabs/suitcaseluggage-dataset/\n",
    "7.   gaurav, gaurav. (2020, July 2). Pytorch Tutorials. GitHub. https://github.com/gaurav67890/Pytorch_Tutorials/blob/master/cnn-scratch-training.ipynb\n",
    "8.   Goodwin, A. (2023, December 7). How to extract text from images using Python. Extract Text From Images With Python. https://pdf.wondershare.com/ocr/extracting-text-from-image-python.html\n",
    "9.   Google, G. (2023). How Many Epochs. Google search. https://www.google.com/search?q=if%2Bwe%2Bhave%2Babig%2Bdataset%2Bhow%2Bis%2Bepoch%2Bdetermined&oq=if%2Bwe%2Bhave%2Babig%2Bdataset%2Bhow%2Bis%2Bepoch%2Bdetermined&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCTEwMzU3ajBqN6gCALACAA&sourceid=chrome&ie=UTF-8\n",
    "10.  Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020, September 16). Array programming with {NumPy}. Numpy. https://numpy.org/citing-numpy/#:~:text=If%20NumPy%20has%20been%20significant,Array%20programming%20with%20NumPy.\n",
    "11.  Kang, A. (2016, April 28). Delta introduces innovative baggage tracking process. Delta News Hub. https://news.delta.com/delta-introduces-innovative-baggage-tracking-process\n",
    "12.  Kouidri, A. (2023, October 10). Master text extraction with MMOCR: A comprehensive guide. Ikomia. https://www.ikomia.ai/blog/easy-text-extraction-using-mmocr\n",
    "13.  May, T. (2023, March 16). Lufthansa partners with Sita to automate mishandled baggage reflight. News. https://airportindustry-news.com/lufthansa-partners-with-sita-to-automate-mishandled-baggage-reflight/\n",
    "14.  numbers-parsers, numbers-parsers. (2023, September). Numbers-parser. PyPI. https://pypi.org/project/numbers-parser/\n",
    "15.  Nurfikri, F. (2022, November). How to build optical character recognition (OCR) in Python. Built In. https://builtin.com/data-science/python-ocr\n",
    "16.  PyPi, P. (2023). Pytesseract. PyPI. https://pypi.org/project/pytesseract/\n",
    "17.  Pytorch, P. (2023). Getting started with transforms v2¶. Getting started with transforms v2 - Torchvision main documentation. https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py9}, journal={Getting started with transforms v2 - Torchvision main documentation\n",
    "18.  Pytorch, P. (2023b). Membership available. PyTorch. https://pytorch.org/\n",
    "19.  Tycomac Codes, T. C. (2021, March 15). How to import your own image data set for your deep learning project (tensorflow/pytorch) | tutorial. YouTube. https://www.youtube.com/watch?v=Y33-wDKQs94 \n",
    "20.  Real Python, R. P. (2023, October 12). How to check if a python string contains a substring. https://realpython.com/python-string-contains-substring/ \n",
    "    \n",
    "     Copyright 2021 Jon Connell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
